{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf714ed",
   "metadata": {},
   "source": [
    "# NVIDIA Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a250a20",
   "metadata": {},
   "source": [
    "##Answer Accuracy\n",
    "\n",
    "Answer Accuracy measures the agreement between a model’s response and a reference ground truth for a given question. This is done via two distinct \"LLM-as-a-judge\" prompts that each return a rating (0, 2, or 4). The metric converts these ratings into a [0,1] scale and then takes the average of the two scores from the judges. Higher scores indicate that the model’s answer closely matches the reference.\n",
    "\n",
    "- 0 → The response is inaccurate or does not address the same question as the reference.\n",
    "- 2 → The response partially align with the reference.\n",
    "- 4 → The response exactly aligns with the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73041f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1e0b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import AnswerAccuracy\n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "\n",
    "# Create your sample\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"When was Einstein born?\",\n",
    "    response=\"Albert Einstein was born in 1879.\",\n",
    "    reference=\"Albert Einstein was born in 1879.\"\n",
    ")\n",
    "\n",
    "# Initialize Groq LLM and wrap it\n",
    "groq_llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "evaluator_llm = LangchainLLMWrapper(groq_llm)\n",
    "\n",
    "# Run metric evaluation\n",
    "scorer = AnswerAccuracy(llm=evaluator_llm)\n",
    "score = await scorer.single_turn_ascore(sample)\n",
    "\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd7f7d",
   "metadata": {},
   "source": [
    "##Context Relevance\n",
    "\n",
    "Context Relevance evaluates whether the retrieved_contexts (chunks or passages) are pertinent to the user_input. This is done via two independent \"LLM-as-a-judge\" prompt calls that each rate the relevance on a scale of 0, 1, or 2. The ratings are then converted to a [0,1] scale and averaged to produce the final score. Higher scores indicate that the contexts are more closely aligned with the user's query.\n",
    "\n",
    "- 0 → The retrieved contexts are not relevant to the user’s query at all.\n",
    "- 1 → The contexts are partially relevant.\n",
    "- 2 → The contexts are completely relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5c2f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from ragas.metrics import ContextRelevance\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    user_input=\"When and Where Albert Einstein was born?\",\n",
    "    retrieved_contexts=[\n",
    "        \"Albert Einstein was born March 14, 1879.\",\n",
    "        \"Albert Einstein was born at Ulm, in Württemberg, Germany.\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "scorer1 = ContextRelevance(llm= evaluator_llm)\n",
    "score1 = await scorer1.single_turn_ascore(sample)\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6616f38",
   "metadata": {},
   "source": [
    "##Response Groundedness\n",
    "\n",
    "Response Groundedness measures how well a response is supported or \"grounded\" by the retrieved contexts. It assesses whether each claim in the response can be found, either wholly or partially, in the provided contexts.\n",
    "\n",
    "- 0 → The response is not grounded in the context at all.\n",
    "- 1 → The response is partially grounded.\n",
    "- 2 → The response is fully grounded (every statement can be found or inferred from the retrieved context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fe3bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from ragas.metrics import ResponseGroundedness\n",
    "\n",
    "sample = SingleTurnSample(\n",
    "    response=\"Albert Einstein was born in 1879.\",\n",
    "    retrieved_contexts=[\n",
    "        \"Albert Einstein was born March 14, 1879.\",\n",
    "        \"Albert Einstein was born at Ulm, in Württemberg, Germany.\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "scorer2 = ResponseGroundedness(llm=evaluator_llm)\n",
    "score2 = await scorer2.single_turn_ascore(sample)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324586b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Complete_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
